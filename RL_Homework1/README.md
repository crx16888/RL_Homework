bash:

# 训练代码
main.py
# 查看训练过程
tensorboard --logdir=C:\Users\95718\Desktop\vscode\Program\RL_Homework\RL_Homework1\ppo-to-solve-simple-maze-main\PPO_logs\Maze_v1\tensorboard_logs
# 模型保存
C:\Users\95718\Desktop\vscode\Program\RL_Homework\RL_Homework1\PPO_preTrained\Maze_v1\PPO_Maze_v1_0_0.pth
# 游戏展示
show.py函数
# 项目代码参考
https://github.com/pigBond/ppo-to-solve-simple-maze






## 实例讲解

让我们使用走迷宫的例子来详细讲解PPO算法的可能训练过程。想象一个简单的迷宫，目标是从起点到达终点，迷宫中有固定的障碍物。智能体可以采取的动作有四种：向上走、向下走、向左走、向右走。为了简化，我们假设每走一步得到的即时奖励是-1（鼓励智能体尽快到达终点），到达终点的奖励是+20，撞到墙壁则为-5（作为惩罚）。(PS:本项目实际操作时对奖励的设置进行了调整)
1. **初始探索**：
   - 智能体在初始阶段随机探索，例如它尝试向左走一步。
   - 如果向左走撞到了墙壁，它会得到-5的奖励。这时，PPO算法会收集这一状态、动作和奖励的信息。
2. **计算优势**：
   - 根据收集到的数据，PPO算法会计算每个动作的优势。在这个例子中，向左走撞墙的动作，相比其他可能的动作（比如向右走可能会接近终点），会被认为是一个低优势动作。
3. **策略更新**：
   - 利用收集到的数据和计算出的优势，PPO开始更新策略。对于撞墙这种低优势的动作，PPO会调整策略，降低未来选择这一动作的概率。
   - 在策略更新过程中，PPO会保证更新的步伐不会过大，即使发现撞墙的动作非常不利，也不会立即将向左走的概率降到极低，而是通过策略比率的截断来温和调整。
4. **进一步探索与学习**：
   - 随着更多的探索和学习，智能体开始理解向着终点方向移动会获得更高的累计奖励。例如，它发现向右走能够避开障碍物，并且逐渐接近终点。
   - 在这个过程中，PPO算法不断优化策略，提高向着终点方向移动的动作概率。
5. **战略优化**：
   - 经过一系列的策略迭代更新后，智能体学会了一个高效的策略来完成迷宫任务。它能够根据当前位置，动态选择最优的动作来最快地达到终点。
   - 在这个过程中，智能体可能会发现一些“捷径”——这些特定的动作序列能够更快地到达终点，这些都是通过PPO算法不断优化和探索得到的。



