# PPO迷宫智能体训练

本项目使用PPO（Proximal Policy Optimization）算法训练一个能够在连续状态空间迷宫中导航的智能体。

## 环境说明

- **状态空间**：二维连续空间，s ∈ R^2，s ∈ [-0.5, 0.5]^2
- **动作空间**：二维连续空间，a ∈ R^2，a ∈ [-0.1, 0.1]^2
- **奖励设计**：
  - 距离目标越近，奖励越高
  - 碰到障碍物有负奖励
  - 接近目标有额外奖励

## 项目结构

- `maze_env.py`: 连续状态空间迷宫环境的实现
- `ppo.py`: PPO算法的实现，包括策略网络、价值网络和优化过程
- `train.py`: 训练和测试脚本
- `logs/`: TensorBoard日志目录
- `models/`: 保存训练模型的目录

## 依赖库

```
torch
numpy
matplotlib
gym
tensorboard
```

## 使用方法

### 训练智能体

```bash
python train.py
```

训练过程中会实时显示迷宫环境和智能体的移动轨迹，并将训练日志保存到`logs`目录。

### 测试智能体

```bash
python train.py --test --model models/ppo_maze_best.pth
```

如果不指定模型路径，将自动使用最新保存的模型进行测试。

### 查看训练曲线

```bash
tensorboard --logdir=logs
```

然后在浏览器中打开http://localhost:6006查看训练曲线。

## 算法说明

本项目使用PPO算法，具有以下特点：

1. **连续动作空间**：使用对角高斯分布表示策略
2. **广义优势估计(GAE)**：平衡偏差和方差
3. **策略裁剪**：限制策略更新幅度，提高稳定性
4. **多次策略更新**：每批数据可以多次使用
5. **熵正则化**：鼓励探索

## 实验结果

训练过程中会生成奖励曲线图，保存在`logs`目录下。最佳模型会保存在`models`目录中。