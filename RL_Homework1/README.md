# PPO迷宫智能体训练

本项目使用PPO（Proximal Policy Optimization）算法训练一个能够在连续状态空间迷宫中导航的智能体。

## 环境说明

- **状态空间**：二维连续空间，s ∈ R^2，s ∈ [-0.5, 0.5]^2
- **动作空间**：二维连续空间，a ∈ R^2，a ∈ [-0.1, 0.1]^2
- **奖励设计**：
  - 距离目标越近，奖励越高
  - 碰到障碍物有负奖励
  - 接近目标有额外奖励

## 项目结构

- `maze_env.py`: 连续状态空间迷宫环境的实现
- `ppo.py`: PPO算法的实现，包括策略网络、价值网络和优化过程
- `train.py`: 训练和测试脚本
- `logs/`: TensorBoard日志目录
- `models/`: 保存训练模型的目录

## 依赖库

```
torch
numpy
matplotlib
gym
tensorboard
```

## 使用方法

### 训练智能体

```bash
python train.py
```

训练过程中会实时显示迷宫环境和智能体的移动轨迹，并将训练日志保存到`logs`目录。

### 测试智能体

```bash
python train.py --test --model models/ppo_maze_best.pth
```

如果不指定模型路径，将自动使用最新保存的模型进行测试。

### 查看训练曲线

```bash
tensorboard --logdir=C:\Users\95718\Desktop\vscode\Program\RL_Homework\RL_Homework1\logs
```

然后在浏览器中打开http://localhost:6006查看训练曲线。

## 算法说明

本项目使用PPO算法，具有以下特点：

1. **连续动作空间**：使用对角高斯分布表示策略
2. **广义优势估计(GAE)**：平衡偏差和方差
3. **策略裁剪**：限制策略更新幅度，提高稳定性
4. **多次策略更新**：每批数据可以多次使用
5. **熵正则化**：鼓励探索

## 实验结果

训练过程中会生成奖励曲线图，保存在`logs`目录下。最佳模型会保存在`models`目录中。


# 一些调参感受
1.尝试思考如下：1. 刚开始的奖励设置没有考虑和障碍物距离相关的惩罚，仅仅就是碰到障碍物给惩罚；最后测试出来的结果这样。智能体虽然在训练的时候表现正常，但是训练的时候策略带有随机探索，在测试的时候取PPO的均值就总体倾向是往右上了。2.后面奖励函数设置加了和障碍物距离相关的惩罚项，得到的效果就是测试的时候智能体到终点的附近但是不到终点了。猜测是没有平衡好惩罚项和奖励项的关系，智能体误以为这个点就是最好的点。3.1) 终点附近的奖励梯度不够陡峭，使得智能体难以区分接近终点和到达终点的状态；2) 障碍物惩罚可能过强，导致智能体过度避障而不敢接近终点；3) 距离奖励与终点奖励之间的平衡不合理，使得智能体在终点附近找到了局部最优解。我需要调整终点奖励值，减小障碍物惩罚的影响范围，并优化奖励函数的梯度，确保智能体能够顺利到达终点。 4.根据测试结果，我发现智能体仍然没有很好地找到目标。我需要进一步优化奖励函数，特别是：1) 进一步增强终点附近的奖励梯度；2) 调整障碍物惩罚与目标奖励的平衡；3) 优化距离奖励的计算方式。 5.增加碰到地图边界的惩罚，和碰到障碍物相同 6.奖励函数设置不要那么复杂，简单点
2.和zsy讨论后的思考：1.碰到障碍物or边界后给负奖励然后不要返回当前点，而是直接给done；没有合适的结束机制导致样本效率太低，然后一直到不了终点学不到到终点的正确奖励 2.得引导智能体一步一步走到终点，而不是只在终点给奖励，你这样太离散了，智能体学不来；最好每一步都有一个跟距离有关的奖励，这样连续一点
3.先把之前有一版的图当作作业交上去了，其他再说，下周有时间再更新
4.过于复杂的奖励函数可能会使学习变得困难，要先理解原始奖励函数的设计意图，然后逐步简化和优化